{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for MNIST database recognizing\n",
    "\n",
    "This is a Neural Network developed without Machine Learning frameworks, using `numpy` and `matplotlib`, only for learning purposes. The objective of this project is practice Neural Networks Design principles and enhance knowledges on Machine Learning and Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Library imports\n",
    "\n",
    "Using `numpy`, `matplotlib`, `mnist` (Anaconda version), `os` and `enum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(all='warn')\n",
    "\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from enum import Enum, unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - MNIST Datasets download\n",
    "\n",
    "Datasets download made with `mnist` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.isdir(\"./datasets/\")):\n",
    "    os.mkdir(\"./datasets\")\n",
    "if(not os.path.isfile(\"./datasets/train-images-idx3-ubyte.gz\")):\n",
    "    mnist.download_file(\"train-images-idx3-ubyte.gz\", \"./datasets\")\n",
    "if(not os.path.isfile(\"./datasets/train-labels-idx1-ubyte.gz\")):\n",
    "    mnist.download_file(\"train-labels-idx1-ubyte.gz\", \"./datasets\")\n",
    "if(not os.path.isfile(\"./datasets/t10k-images-idx3-ubyte.gz\")):\n",
    "    mnist.download_file(\"t10k-images-idx3-ubyte.gz\", \"./datasets\")\n",
    "if(not os.path.isfile(\"./datasets/t10k-labels-idx1-ubyte.gz\")):\n",
    "    mnist.download_file(\"t10k-labels-idx1-ubyte.gz\", \"./datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - MNIST Datasets Load\n",
    "\n",
    "Load and reshaping of the datasets. Labels converted to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_raw = mnist.train_images()\n",
    "train_labels_raw = mnist.train_labels()\n",
    "test_images_raw = mnist.test_images()\n",
    "test_labels_raw = mnist.test_labels()\n",
    "\n",
    "train_images = np.transpose(train_images_raw.reshape(train_images_raw.shape[0], train_images_raw.shape[1]*train_images_raw.shape[2]))\n",
    "\n",
    "train_labels = np.zeros((train_labels_raw.shape[0], train_labels_raw.max()+1))\n",
    "train_labels[np.arange(train_labels_raw.shape[0]), train_labels_raw] = 1\n",
    "train_labels = np.transpose(train_labels)\n",
    "\n",
    "test_images = np.transpose(test_images_raw.reshape(test_images_raw.shape[0], test_images_raw.shape[1]*test_images_raw.shape[2]))\n",
    "\n",
    "test_labels = np.zeros((test_labels_raw.shape[0], test_labels_raw.max()+1))\n",
    "test_labels[np.arange(test_labels_raw.shape[0]), test_labels_raw] = 1\n",
    "test_labels = np.transpose(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Activation functions enum\n",
    "\n",
    "Activation enum. It's used to select whether activation function will be applied on each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unique\n",
    "class Activation(Enum):\n",
    "    SIGMOID = 1\n",
    "    TANH = 2\n",
    "    RELU = 3\n",
    "    LEAKY_RELU = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Hyperparameters\n",
    "\n",
    "Hyperparameter constants. It's adjusted and applied on functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = train_images.shape[0]\n",
    "\n",
    "LAYER_UNITS = np.array([32, 32, 16, 10], dtype=np.uint32)\n",
    "LAYER_ACTIVATIONS = np.array([Activation['RELU'], Activation['RELU'], Activation['TANH'], Activation['SIGMOID']])\n",
    "LAYERS = LAYER_UNITS.shape[0]\n",
    "ALPHA = 10e-3\n",
    "LAMBDA_REG = 10e-8\n",
    "EPOCHS = 100\n",
    "EPSILON = 10e-8\n",
    "MINIBATCH_SIZE = 512\n",
    "LEARNING_DECAY = 0.95\n",
    "BETA_MOMENTUM = 0.9\n",
    "BETA_RMS = 0.999\n",
    "\n",
    "EXAMPLES = train_images.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Activation functions and it's derivatives\n",
    "\n",
    "Linear function, Sigmoid, Tanh, ReLU and Leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAKY_RELU_MULTIPLIER = 0.01\n",
    "\n",
    "def linear_func(X_matrix, W_matrix, b_array):\n",
    "    return np.dot(W_matrix, X_matrix) + b_array\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_threshold(z):\n",
    "    return np.round(z)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def leaky_relu(z):\n",
    "    return np.maximum(LEAKY_RELU_MULTIPLIER * z, z)\n",
    "\n",
    "def derivative_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def derivative_tanh(z):\n",
    "    return 1 - (tanh(z)**2)\n",
    "\n",
    "def derivative_relu(z):\n",
    "    if(z < 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def derivative_leaky_relu(z):\n",
    "    if(z < 0):\n",
    "        return LEAKY_RELU_MULTIPLIER\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "derivative_relu = np.vectorize(derivative_relu)\n",
    "derivative_leaky_relu = np.vectorize(derivative_leaky_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Normalization functions\n",
    "\n",
    "For some reason, normalization with mean and standard was buggy, so... I just divided each feature by it's maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(X_matrix):\n",
    "    X_norm = np.zeros(X_matrix.shape)\n",
    "    for i in range(X_matrix.shape[0]):\n",
    "        X_norm[i] = X_matrix[i]/np.maximum(1, np.max(X_matrix[i]))\n",
    "    return X_norm\n",
    "\n",
    "def normalize_input(x_input, X_matrix):\n",
    "    x_norm = np.zeros(x_input.shape)\n",
    "    x_norm = x_input/np.maximum(1, np.max(X_matrix))\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Forward and Backward Propagation Functions\n",
    "\n",
    "Fwdprop and backprop functions (optimizations are done in fit function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_prop(A_previous, W_layer, b_layer, activationType = Activation['SIGMOID']):\n",
    "    Z_layer = linear_func(A_previous, W_layer, b_layer)\n",
    "    if (activationType == Activation['SIGMOID']):\n",
    "        A_layer = sigmoid(Z_layer)\n",
    "    elif (activationType == Activation['TANH']):\n",
    "        A_layer = tanh(Z_layer)\n",
    "    elif (activationType == Activation['RELU']):\n",
    "        A_layer = relu(Z_layer)\n",
    "    elif (activationType == Activation['LEAKY_RELU']):\n",
    "        A_layer = leaky_relu(Z_layer)\n",
    "    else:\n",
    "        A_layer = sigmoid(Z_layer)\n",
    "    return A_layer, Z_layer\n",
    "\n",
    "def back_prop(dA_layer, A_previous, Z_layer, W_layer, b_layer, activationType = Activation['SIGMOID']):\n",
    "    if (activationType == Activation['SIGMOID']):\n",
    "        dZ_layer = np.multiply(dA_layer, derivative_sigmoid(Z_layer))\n",
    "    elif (activationType == Activation['TANH']):\n",
    "        dZ_layer = np.multiply(dA_layer, derivative_tanh(Z_layer))\n",
    "    elif (activationType == Activation['RELU']):\n",
    "        dZ_layer = np.multiply(dA_layer, derivative_relu(Z_layer))\n",
    "    elif (activationType == Activation['LEAKY_RELU']):\n",
    "        dZ_layer = np.multiply(dA_layer, derivative_leaky_relu(Z_layer))\n",
    "    else:\n",
    "        dZ_layer = np.multiply(dA_layer, derivative_sigmoid(Z_layer))\n",
    "    \n",
    "    dW_layer = np.dot(dZ_layer, np.transpose(A_previous))/dZ_layer.shape[1]\n",
    "    db_layer = np.sum(dZ_layer, axis = 1, keepdims = True)/dZ_layer.shape[1]\n",
    "    dA_previous = np.dot(np.transpose(W_layer), dZ_layer)\n",
    "\n",
    "    return dA_previous, dW_layer, db_layer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Predict Function\n",
    "\n",
    "Prediction based on calculated weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b,\n",
    "            layers = LAYERS,\n",
    "            layer_activations = LAYER_ACTIVATIONS):\n",
    "    A = X\n",
    "    for i in range(1, layers+1):\n",
    "        A, z = fwd_prop(A, W[i], b[i], layer_activations[i-1])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - Cost function\n",
    "\n",
    "For some reason, $- y \\cdot \\text{log}(y_{prediction}) - (1 - y) \\cdot \\text{log}(1 - y_{prediction})$ didn't work so well, so I used mean squared error instead.\n",
    "\n",
    "This cost function doesn't affect neither the propagation functions, nor the fit function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, y, W, b, lambda_reg = LAMBDA_REG):\n",
    "    y_prediction = predict(X, W, b)\n",
    "    regularization = 0\n",
    "    loss = (1/2)*((y_prediction - y)**2)\n",
    "    for i in W:\n",
    "        regularization += np.sum(i**2)\n",
    "    cost = (np.sum(loss, axis = 1, keepdims=True)/y.shape[1]) + ((lambda_reg/(2*y.shape[1]))*regularization)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - Fit Function\n",
    "\n",
    "Fit function is implemented with optimization. (I'm using Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y,\n",
    "        W_previous = None,\n",
    "        b_previous = None,\n",
    "        features = FEATURES,\n",
    "        layers = LAYERS,\n",
    "        layer_units = LAYER_UNITS,\n",
    "        layer_activations = LAYER_ACTIVATIONS,\n",
    "        examples = EXAMPLES,\n",
    "        alpha = ALPHA,\n",
    "        learning_decay = LEARNING_DECAY,\n",
    "        lambda_reg = LAMBDA_REG,\n",
    "        epochs = EPOCHS,\n",
    "        epsilon = EPSILON,\n",
    "        mb_size = MINIBATCH_SIZE,\n",
    "        beta_momentum = BETA_MOMENTUM,\n",
    "        beta_rms = BETA_RMS):\n",
    "    \n",
    "    W = {1: np.random.randn(layer_units[0], features) * np.sqrt(2/features)}\n",
    "    dW = {1: np.zeros([layer_units[0], features])}\n",
    "    b = {1:np.random.randn(layer_units[0], 1)}\n",
    "    db = {1: np.zeros([layer_units[0], 1])}\n",
    "    Z = {0: X}\n",
    "    A = {0: X}\n",
    "    dA = {0: np.array([])}\n",
    "    for k in range(layers - 1):\n",
    "        W[k+2] = np.random.randn(layer_units[k+1], layer_units[k]) * np.sqrt(1/layer_units[k])\n",
    "        dW[k+2] = np.zeros([layer_units[k+1], layer_units[k]])\n",
    "        b[k+2] = np.random.randn(layer_units[k+1], 1)\n",
    "        db[k+2] = np.zeros([layer_units[k+1], 1])\n",
    "        Z[k+1] = np.zeros([layer_units[k+1], examples])\n",
    "        A[k+1] = np.zeros([layer_units[k+1], examples])\n",
    "        dA[k+1] = np.zeros([layer_units[k+1], examples])\n",
    "\n",
    "    if(W_previous != None and b_previous != None):\n",
    "        W = W_previous\n",
    "        b = b_previous\n",
    "\n",
    "    alpha_decay = alpha\n",
    "\n",
    "    cost_points = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        vdW = {}\n",
    "        vdb = {}\n",
    "        sdW = {}\n",
    "        sdb = {}\n",
    "        for v in range(1, layers+1):\n",
    "            vdW[v] = np.zeros(dW[v].shape)\n",
    "            vdb[v] = np.zeros(db[v].shape)\n",
    "            sdW[v] = np.zeros(dW[v].shape)\n",
    "            sdb[v] = np.zeros(db[v].shape)\n",
    "\n",
    "        print(f\"\\rEpoch {i+1} of {epochs}\", end=\"\")\n",
    "\n",
    "        for k in range(int(np.ceil(A[0].shape[1]/mb_size))):\n",
    "\n",
    "            mb_start = k*mb_size\n",
    "            mb_end = np.minimum((k+1)*mb_size, A[0].shape[1])\n",
    "\n",
    "            for j in range(layers):\n",
    "                if(j == 0):\n",
    "                    A[j+1], Z[j+1] = fwd_prop(A[j][:, mb_start:mb_end], W[j+1], b[j+1], layer_activations[j])\n",
    "                else:\n",
    "                    A[j+1], Z[j+1] = fwd_prop(A[j], W[j+1], b[j+1], layer_activations[j])\n",
    "\n",
    "            dA[layers] = - (y[:, mb_start:mb_end]/A[layers]) + ((1-y[:, mb_start:mb_end])/(1-A[layers]))\n",
    "\n",
    "            for j in range(layers-1, -1, -1):\n",
    "                if(j == 0):\n",
    "                    dA[j], dW[j+1], db[j+1] = back_prop(dA[j+1], A[j][:, mb_start:mb_end], Z[j+1], W[j+1], b[j+1], layer_activations[j])\n",
    "                else:\n",
    "                    dA[j], dW[j+1], db[j+1] = back_prop(dA[j+1], A[j], Z[j+1], W[j+1], b[j+1], layer_activations[j])\n",
    "            \n",
    "            for j in range(1, layers+1):\n",
    "                vdW[v] = ((beta_momentum * vdW[v]) + ((1 - beta_momentum) * dW[v]))/(1 + (beta_momentum**(k+1)))\n",
    "                vdb[v] = ((beta_momentum * vdb[v]) + ((1 - beta_momentum) * db[v]))/(1 + (beta_momentum**(k+1)))\n",
    "                sdW[v] = ((beta_rms * sdW[v]) + ((1 - beta_rms) * dW[v]**2))/(1 + (beta_rms**(k+1)))\n",
    "                sdb[v] = ((beta_rms * sdb[v]) + ((1 - beta_rms) * db[v]**2))/(1 + (beta_rms**(k+1)))\n",
    "                W[j] = (W[j] * (1 - ((alpha_decay*lambda_reg)/y.shape[1]))) - (alpha_decay*(vdW[j]/(epsilon + np.sqrt(sdW[j]))))\n",
    "                b[j] = b[j] - (alpha_decay*(vdb[j]/(epsilon + np.sqrt(sdb[j]))))\n",
    "    \n",
    "            cost_points.append(cost(X[:, mb_start:mb_end], y[:, mb_start:mb_end], W, b))\n",
    "        \n",
    "        alpha_decay = alpha*(learning_decay**(i+1))\n",
    "\n",
    "        #print(np.transpose(cost_points[len(cost_points) - 1]))\n",
    "    \n",
    "    print(\"\\r                        \", end=\"\")\n",
    "    print(\"\\rDone.\")\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 - Training the N.N.\n",
    "\n",
    "Running fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.                   \n"
     ]
    }
   ],
   "source": [
    "train_images_normalized = normalize_dataset(train_images)\n",
    "W_final, b_final = fit(train_images_normalized, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 - Saving Checkpoint\n",
    "\n",
    "Here we can save the results of fitting. We can run a new fit not losing previous fit in `W_save` and `b_save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_save = W_final\n",
    "b_save = b_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `ACTIVE_SAVING_CHECKPOINT` == `True`, `W_save` and `b_save` are saved on `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVE_SAVING_CHECKPOINT = False\n",
    "\n",
    "if(ACTIVE_SAVING_CHECKPOINT):\n",
    "    if(not os.path.isdir(\"./training-checkpoint/\")):\n",
    "        os.mkdir(\"./training-checkpoint/\")\n",
    "\n",
    "    for i in W_save:\n",
    "        np.savetxt(f'./training-checkpoint/w{i}.csv', W_save[i], delimiter=',')\n",
    "    for i in b_save:\n",
    "        np.savetxt(f'./training-checkpoint/b{i}.csv', b_save[i], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14 - Loading Checkpoint\n",
    "\n",
    "If `ACTIVE_LOADING_CHECKPOINT` == `True`, `W_save` and `b_save` are loaded with `.csv` files content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVE_LOADING_CHECKPOINT = False\n",
    "\n",
    "if(ACTIVE_LOADING_CHECKPOINT):\n",
    "    W_save = {}\n",
    "    b_save = {}\n",
    "    for i in range(1, LAYERS + 1):\n",
    "        W_save[i] = np.loadtxt(f'./training-checkpoint/w{i}.csv', delimiter=',')\n",
    "        b_save[i] = np.loadtxt(f'./training-checkpoint/b{i}.csv', delimiter=',')\n",
    "        b_save[i] = np.reshape(b_save[i], (b_save[i].shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 - Measuring accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set prediction: 14.87 %\n"
     ]
    }
   ],
   "source": [
    "test_images_normalized = normalize_input(test_images, train_images)\n",
    "test_outputs = np.transpose(sigmoid_threshold(np.transpose(predict(test_images_normalized, W_save, b_save))))\n",
    "test_comparison = np.array([np.array_equal(test_outputs[:, i], test_labels[:, i]) for i in range(test_outputs.shape[1])])\n",
    "test_accuracy = np.sum(test_comparison)*100/test_comparison.shape[0]\n",
    "print(f\"Accuracy on test set prediction: {test_accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16 - Measuring accuracy on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set prediction: 14.17 %\n"
     ]
    }
   ],
   "source": [
    "train_images_normalized = normalize_dataset(train_images)\n",
    "train_outputs = np.transpose(sigmoid_threshold(np.transpose(predict(train_images_normalized, W_save, b_save))))\n",
    "train_comparison = np.array([np.array_equal(train_outputs[:, i], train_labels[:, i]) for i in range(train_outputs.shape[1])])\n",
    "train_accuracy = np.sum(train_comparison)*100/train_comparison.shape[0]\n",
    "print(f\"Accuracy on train set prediction: {train_accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17 - Test playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída esperada: 2\n",
      "Sem predição válida.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = int(np.round(np.random.rand()*len(test_images_raw)))\n",
    "\n",
    "plt.imshow(test_images_raw[index], cmap='magma')\n",
    "test_sample = normalize_input(test_images[:, index].reshape(test_images[:, index].shape[0], 1), train_images)\n",
    "test_predict = sigmoid_threshold(np.transpose(predict(test_sample, W_save, b_save)))\n",
    "print(f\"Saída esperada: {test_labels_raw[index]}\")\n",
    "for i in range(len(test_predict[0])):\n",
    "    if test_predict[0, i] == 1:\n",
    "        print(f\"Saída predita: {i}\")\n",
    "        break\n",
    "    if(i == 9):\n",
    "        print(f\"Sem predição válida.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "427b8541cfc07e6fbe7ab4a5298567b1b3022ff2b70fdb07d029f33f0434686a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
