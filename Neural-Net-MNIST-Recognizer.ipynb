{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for MNIST database recognizing\n",
    "\n",
    "This is a Neural Network developed without Machine Learning frameworks, using `numpy` and `matplotlib`, only for learning purposes. The objective of this project is practice Neural Networks Design principles and enhance knowledges on Machine Learning and Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(all='warn')\n",
    "\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from enum import Enum, unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - MNIST Datasets download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.isdir(\"./datasets/\")):\n",
    "    os.mkdir(\"./datasets\")\n",
    "if(not os.path.isfile(\"./datasets/train-images-idx3-ubyte.gz\")):\n",
    "    mnist.download_file(\"train-images-idx3-ubyte.gz\", \"./datasets\")\n",
    "if(not os.path.isfile(\"./datasets/train-labels-idx1-ubyte.gz\")):\n",
    "    mnist.download_file(\"train-labels-idx1-ubyte.gz\", \"./datasets\")\n",
    "if(not os.path.isfile(\"./datasets/t10k-images-idx3-ubyte.gz\")):\n",
    "    mnist.download_file(\"t10k-images-idx3-ubyte.gz\", \"./datasets\")\n",
    "if(not os.path.isfile(\"./datasets/t10k-labels-idx1-ubyte.gz\")):\n",
    "    mnist.download_file(\"t10k-labels-idx1-ubyte.gz\", \"./datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - MNIST Datasets Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_raw = mnist.train_images()\n",
    "train_labels_raw = mnist.train_labels()\n",
    "test_images_raw = mnist.test_images()\n",
    "test_labels_raw = mnist.test_labels()\n",
    "\n",
    "train_images = np.transpose(train_images_raw.reshape(train_images_raw.shape[0], train_images_raw.shape[1]*train_images_raw.shape[2]))\n",
    "\n",
    "train_labels = np.zeros((train_labels_raw.shape[0], train_labels_raw.max()+1))\n",
    "train_labels[np.arange(train_labels_raw.shape[0]), train_labels_raw] = 1\n",
    "train_labels = np.transpose(train_labels)\n",
    "\n",
    "test_images = np.transpose(test_images_raw.reshape(test_images_raw.shape[0], test_images_raw.shape[1]*test_images_raw.shape[2]))\n",
    "\n",
    "test_labels = np.zeros((test_labels_raw.shape[0], test_labels_raw.max()+1))\n",
    "test_labels[np.arange(test_labels_raw.shape[0]), test_labels_raw] = 1\n",
    "test_labels = np.transpose(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Activation functions enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unique\n",
    "class Activation(Enum):\n",
    "    SIGMOID = 1\n",
    "    TANH = 2\n",
    "    RELU = 3\n",
    "    LEAKY_RELU = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = train_images.shape[0]\n",
    "LAYERS = 3\n",
    "LAYER_UNITS = np.array([16, 10, 10], dtype=np.uint32)\n",
    "LAYER_ACTIVATIONS = np.array([Activation['RELU'], Activation['TANH'], Activation['SIGMOID']])\n",
    "ALPHA = 7*10e-2\n",
    "LAMBDA_REG = 10e-4\n",
    "ITERATIONS = 300\n",
    "EPSILON = 0.05\n",
    "MINIBATCH_SIZE = 1000\n",
    "\n",
    "EXAMPLES = train_images.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Activation functions and it's derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAKY_RELU_MULTIPLIER = 0.01\n",
    "\n",
    "def linear_func(X_matrix, W_matrix, b_array):\n",
    "    return np.dot(W_matrix, X_matrix) + b_array\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_threshold(z):\n",
    "    return np.round(z)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def leaky_relu(z):\n",
    "    return np.maximum(LEAKY_RELU_MULTIPLIER * z, z)\n",
    "\n",
    "def derivative_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def derivative_tanh(z):\n",
    "    return 1 - (tanh(z)**2)\n",
    "\n",
    "def derivative_relu(z):\n",
    "    if(z < 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def derivative_leaky_relu(z):\n",
    "    if(z < 0):\n",
    "        return LEAKY_RELU_MULTIPLIER\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "derivative_relu = np.vectorize(derivative_relu)\n",
    "derivative_leaky_relu = np.vectorize(derivative_leaky_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(X_matrix):\n",
    "    X_norm = np.zeros(X_matrix.shape)\n",
    "    for i in range(X_matrix.shape[0]):\n",
    "        X_norm[i] = X_matrix[i]/np.maximum(1, np.max(X_matrix[i]))\n",
    "    return X_norm\n",
    "\n",
    "def normalize_input(x_input, X_matrix):\n",
    "    x_norm = np.zeros(x_input.shape)\n",
    "    x_norm = x_input/np.maximum(1, np.max(X_matrix))\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Forward and Backward Propagation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_prop(A_previous, W_layer, b_layer, activationType = Activation['SIGMOID']):\n",
    "    Z_layer = linear_func(A_previous, W_layer, b_layer)\n",
    "    if (activationType == Activation['SIGMOID']):\n",
    "        A_layer = sigmoid(Z_layer)\n",
    "    elif (activationType == Activation['TANH']):\n",
    "        A_layer = tanh(Z_layer)\n",
    "    elif (activationType == Activation['RELU']):\n",
    "        A_layer = relu(Z_layer)\n",
    "    elif (activationType == Activation['LEAKY_RELU']):\n",
    "        A_layer = leaky_relu(Z_layer)\n",
    "    else:\n",
    "        A_layer = sigmoid(Z_layer)\n",
    "    return A_layer, Z_layer\n",
    "\n",
    "def back_prop(dA_layer, A_previous, Z_layer, W_layer, b_layer, activationType = Activation['SIGMOID']):\n",
    "    if (activationType == Activation['SIGMOID']):\n",
    "        dZ_layer = np.multiply(dA_layer, derivative_sigmoid(Z_layer))\n",
    "    elif (activationType == Activation['TANH']):\n",
    "        dZ_layer = np.multiply(dA_layer, derivative_tanh(Z_layer))\n",
    "    elif (activationType == Activation['RELU']):\n",
    "        dZ_layer = np.multiply(dA_layer, derivative_relu(Z_layer))\n",
    "    elif (activationType == Activation['LEAKY_RELU']):\n",
    "        dZ_layer = np.multiply(dA_layer, derivative_leaky_relu(Z_layer))\n",
    "    else:\n",
    "        dZ_layer = np.multiply(dA_layer, derivative_sigmoid(Z_layer))\n",
    "    \n",
    "    dW_layer = np.dot(dZ_layer, np.transpose(A_previous))/dZ_layer.shape[1]\n",
    "    db_layer = np.sum(dZ_layer, axis = 1, keepdims = True)/dZ_layer.shape[1]\n",
    "    dA_previous = np.dot(np.transpose(W_layer), dZ_layer)\n",
    "\n",
    "    return dA_previous, dW_layer, db_layer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b,\n",
    "            layers = LAYERS,\n",
    "            layer_activations = LAYER_ACTIVATIONS):\n",
    "    A = X\n",
    "    for i in range(1, layers+1):\n",
    "        A, z = fwd_prop(A, W[i], b[i], layer_activations[i-1])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, y, W, b, lambda_reg = LAMBDA_REG):\n",
    "    y_prediction = predict(X, W, b)\n",
    "    regularization = 0\n",
    "    loss = (1/2)*((y_prediction - y)**2)\n",
    "    for i in W:\n",
    "        regularization += np.sum(i**2)\n",
    "    cost = (np.sum(loss, axis = 1, keepdims=True)/y.shape[1]) + ((lambda_reg/(2*y.shape[1]))*regularization)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - Fit Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y,\n",
    "        W_previous = None,\n",
    "        b_previous = None,\n",
    "        features = FEATURES,\n",
    "        layers = LAYERS,\n",
    "        layer_units = LAYER_UNITS,\n",
    "        layer_activations = LAYER_ACTIVATIONS,\n",
    "        examples = EXAMPLES,\n",
    "        alpha = ALPHA,\n",
    "        lambda_reg = LAMBDA_REG,\n",
    "        iterations = ITERATIONS,\n",
    "        epsilon = EPSILON,\n",
    "        mb_size = MINIBATCH_SIZE):\n",
    "    \n",
    "    W = {1: np.random.randn(layer_units[0], features) * np.sqrt(2/features)}\n",
    "    dW = {1: np.zeros([layer_units[0], features])}\n",
    "    b = {1:np.random.randn(layer_units[0], 1)}\n",
    "    db = {1: np.zeros([layer_units[0], 1])}\n",
    "    Z = {0: X}\n",
    "    A = {0: X}\n",
    "    dA = {0: np.array([])}\n",
    "    for k in range(layers - 1):\n",
    "        W[k+2] = np.random.randn(layer_units[k+1], layer_units[k]) * np.sqrt(1/layer_units[k])\n",
    "        dW[k+2] = np.zeros([layer_units[k+1], layer_units[k]])\n",
    "        b[k+2] = np.random.randn(layer_units[k+1], 1)\n",
    "        db[k+2] = np.zeros([layer_units[k+1], 1])\n",
    "        Z[k+1] = np.zeros([layer_units[k+1], examples])\n",
    "        A[k+1] = np.zeros([layer_units[k+1], examples])\n",
    "        dA[k+1] = np.zeros([layer_units[k+1], examples])\n",
    "\n",
    "    if(W_previous != None and b_previous != None):\n",
    "        W = W_previous\n",
    "        b = b_previous\n",
    "\n",
    "    cost_points = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        for k in range(int(A[0].shape[1]/mb_size)):\n",
    "\n",
    "            for j in range(layers):\n",
    "                if(j == 0):\n",
    "                    A[j+1], Z[j+1] = fwd_prop(A[j][:, k*mb_size:(k+1)*mb_size], W[j+1], b[j+1], layer_activations[j])\n",
    "                else:\n",
    "                    A[j+1], Z[j+1] = fwd_prop(A[j], W[j+1], b[j+1], layer_activations[j])\n",
    "\n",
    "            dA[layers] = - (y[:, k*mb_size:(k+1)*mb_size]/A[layers]) + ((1-y[:, k*mb_size:(k+1)*mb_size])/(1-A[layers]))\n",
    "\n",
    "            for j in range(layers-1, -1, -1):\n",
    "                if(j == 0):\n",
    "                    dA[j], dW[j+1], db[j+1] = back_prop(dA[j+1], A[j][:, k*mb_size:(k+1)*mb_size], Z[j+1], W[j+1], b[j+1], layer_activations[j])\n",
    "                else:\n",
    "                    dA[j], dW[j+1], db[j+1] = back_prop(dA[j+1], A[j], Z[j+1], W[j+1], b[j+1], layer_activations[j])\n",
    "            \n",
    "            for j in range(1, layers+1):\n",
    "                W[j] = (W[j] * (1 - ((alpha*lambda_reg)/y.shape[1]))) - (alpha*dW[j])\n",
    "                b[j] = b[j] - (alpha*db[j])\n",
    "    \n",
    "            cost_points.append(cost(X[:, k*mb_size:(k+1)*mb_size], y[:, k*mb_size:(k+1)*mb_size], W, b))\n",
    "        \n",
    "        print(f\"\\rEpoch {i+1} of {iterations}\", end=\"\")\n",
    "        #print(np.transpose(cost_points[len(cost_points) - 1]))\n",
    "    \n",
    "    print(\"\\r                        \", end=\"\")\n",
    "    print(\"\\rDone.\")\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 - Training the N.N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.                   \n"
     ]
    }
   ],
   "source": [
    "train_images_normalized = normalize_dataset(train_images)\n",
    "W_final, b_final = fit(train_images_normalized, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 - Saving Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_save = W_final\n",
    "b_save = b_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.isdir(\"./training-checkpoint/\")):\n",
    "    os.mkdir(\"./training-checkpoint/\")\n",
    "\n",
    "#checkpoint_file = open(\"./training-checkpoint/checkpoint.txt\", 'w')\n",
    "for i in W_save:\n",
    "    np.savetxt(f'./training-checkpoint/w{i}.csv', W_save[i], delimiter=',')\n",
    "for i in b_save:\n",
    "    np.savetxt(f'./training-checkpoint/b{i}.csv', b_save[i], delimiter=',')\n",
    "#checkpoint_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14 - Loading Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_save = {}\n",
    "b_save = {}\n",
    "for i in range(1, LAYERS + 1):\n",
    "    W_save[i] = np.loadtxt(f'./training-checkpoint/w{i}.csv', delimiter=',')\n",
    "    b_save[i] = np.loadtxt(f'./training-checkpoint/b{i}.csv', delimiter=',')\n",
    "    b_save[i] = np.reshape(b_save[i], (b_save[i].shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 - Measuring accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set prediction: 93.07 %\n"
     ]
    }
   ],
   "source": [
    "test_images_normalized = normalize_input(test_images, train_images)\n",
    "test_outputs = np.transpose(sigmoid_threshold(np.transpose(predict(test_images_normalized, W_save, b_save))))\n",
    "test_comparison = np.array([np.array_equal(test_outputs[:, i], test_labels[:, i]) for i in range(test_outputs.shape[1])])\n",
    "test_accuracy = np.sum(test_comparison)*100/test_comparison.shape[0]\n",
    "print(f\"Accuracy on test set prediction: {test_accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16 - Measuring accuracy on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set prediction: 95.585 %\n"
     ]
    }
   ],
   "source": [
    "train_images_normalized = normalize_dataset(train_images)\n",
    "train_outputs = np.transpose(sigmoid_threshold(np.transpose(predict(train_images_normalized, W_save, b_save))))\n",
    "train_comparison = np.array([np.array_equal(train_outputs[:, i], train_labels[:, i]) for i in range(train_outputs.shape[1])])\n",
    "train_accuracy = np.sum(train_comparison)*100/train_comparison.shape[0]\n",
    "print(f\"Accuracy on train set prediction: {train_accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17 - Test playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída esperada: 7\n",
      "Saída predita: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = int(np.round(np.random.rand()*len(test_images_raw)))\n",
    "\n",
    "plt.imshow(test_images_raw[index], cmap='magma')\n",
    "test_sample = normalize_input(test_images[:, index].reshape(test_images[:, index].shape[0], 1), train_images)\n",
    "test_predict = sigmoid_threshold(np.transpose(predict(test_sample, W_save, b_save)))\n",
    "print(f\"Saída esperada: {test_labels_raw[index]}\")\n",
    "for i in range(len(test_predict[0])):\n",
    "    if test_predict[0, i] == 1:\n",
    "        print(f\"Saída predita: {i}\")\n",
    "        break\n",
    "    if(i == 9):\n",
    "        print(f\"Sem predição válida.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "427b8541cfc07e6fbe7ab4a5298567b1b3022ff2b70fdb07d029f33f0434686a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
